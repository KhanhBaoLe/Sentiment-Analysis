{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7578f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a92cc",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec8b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khanh\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c36ab9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = path + \"/IMDB Dataset.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47950570",
   "metadata": {},
   "source": [
    "Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e7243f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7fc5973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][2].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b69094",
   "metadata": {},
   "source": [
    "Tags Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51826288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffaeb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_remover(text):\n",
    "    pattern = re.compile(r'<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f68b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Machine learning is a branch of <b>artificial intelligence</b> that focuses on building <i>models</i> that learn from data.</p> <div><h1>Natural Language Processing</h1><p>NLP helps computers understand <span style=\"color:red;\">human language</span>.</p></div> <ul><li>Tokenization</li><li>Stemming</li><li><a href=\"#\">TF-IDF Calculation</a></li></ul> <footer>Created by Kareem and Mohamed</footer>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '<p>Machine learning is a branch of <b>artificial intelligence</b> that focuses on building <i>models</i> that learn from data.</p> <div><h1>Natural Language Processing</h1><p>NLP helps computers understand <span style=\"color:red;\">human language</span>.</p></div> <ul><li>Tokenization</li><li>Stemming</li><li><a href=\"#\">TF-IDF Calculation</a></li></ul> <footer>Created by Kareem and Mohamed</footer>'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e24bf8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a branch of artificial intelligence that focuses on building models that learn from data. Natural Language ProcessingNLP helps computers understand human language. TokenizationStemmingTF-IDF Calculation Created by Kareem and Mohamed'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_remover(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d863cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3e2d3e",
   "metadata": {},
   "source": [
    "URL Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "781617bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_remover(text):\n",
    "    pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac603019",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check my profile at https://www.linkedin.com/in/user-profile'\n",
    "text2 = 'Download the file from http://example.com/files/download.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "150e1dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check my profile at \n",
      "Download the file from \n"
     ]
    }
   ],
   "source": [
    "print(url_remover(text1))\n",
    "print(url_remover(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39ea66",
   "metadata": {},
   "source": [
    "Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4f718c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a070013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_remover(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43c9423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98df1d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog However the dog doesnt seem impressed Oh no it just yawned How disappointing Maybe a squirrel would elicit a reaction Alas the fox is out of luck'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc_remover(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81015079",
   "metadata": {},
   "source": [
    "Chatwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3d683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09aa733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatwords_converter(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_text.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79b0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'IMHO he is the best'\n",
    "text2 = 'FYI Islamabad is the capital of Pakistan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8595f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n",
      "For Your Information Islamabad is the capital of Pakistan\n"
     ]
    }
   ],
   "source": [
    "print(chatwords_converter(text1))\n",
    "print(chatwords_converter(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cf22e",
   "metadata": {},
   "source": [
    "Spelling issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "810aeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c36a160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "incorrect_text2 = 'The cat sat on the cuchion. while plyaiing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "906f427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb1 = TextBlob(incorrect_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b3c0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.\n",
      "certain conditions during several generations are modified in the same manner.\n",
      "\n",
      "The cat sat on the cuchion. while plyaiing\n",
      "The cat sat on the cushion. while playing\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_text)\n",
    "print(textBlb.correct().string)\n",
    "print()\n",
    "print(incorrect_text2)\n",
    "print(textBlb1.correct().string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf3809",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0d89629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64e764e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87b29c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remover(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.lower() not in stopword:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a374d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b82c3f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably all-time favorite movie, story selflessness, sacrifice dedication noble cause, preachy boring. never gets old, despite seen 15 times'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_remover(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1917c",
   "metadata": {},
   "source": [
    "Handling emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bbde24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acting was amazing üòç \n",
      " This film was so boring üò¥ \n",
      " Absolutely terrible üò°\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The acting was amazing üòç\"\n",
    "text2 = \"This film was so boring üò¥\"\n",
    "text3 = \"Absolutely terrible üò°\"\n",
    "print(text1 ,'\\n', text2 ,'\\n', text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f6ea82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ee80538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acting was amazing \n",
      "This film was so boring \n",
      "Absolutely terrible \n"
     ]
    }
   ],
   "source": [
    "print(remove_emoji(text1))\n",
    "print(remove_emoji(text2))\n",
    "print(remove_emoji(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6de376",
   "metadata": {},
   "source": [
    "Convert Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8169fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\python versions\\lib\\site-packages (2.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecfc68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_converted(text):\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d06ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acting was amazing :smiling_face_with_heart-eyes:\n",
      "This film was so boring :sleeping_face:\n",
      "Absolutely terrible :enraged_face:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize(text1))\n",
    "print(emoji.demojize(text2))\n",
    "print(emoji.demojize(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c4f0b",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14de4d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\khanh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\khanh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "843bc584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'comedy', 'scenes', 'were', 'hilarious', '!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The comedy scenes were hilarious!'\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07b72505",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"For decades, researchers have examined the evolution of written communication, exploring how language adapts to new technologies.\n",
    "As printing methods improved during the early modern period, publishers began experimenting with standardized layouts,\n",
    "creating templates that allowed readers to follow complex information with greater ease. Over time, these conventions shaped\n",
    "the way books, newspapers, and academic materials were produced across different regions. Today, digital platforms continue this\n",
    "tradition, offering new tools that transform how information is shared, accessed, and preserved for future generations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "877d179b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For decades, researchers have examined the evolution of written communication, exploring how language adapts to new technologies.',\n",
       " 'As printing methods improved during the early modern period, publishers began experimenting with standardized layouts,\\ncreating templates that allowed readers to follow complex information with greater ease.',\n",
       " 'Over time, these conventions shaped\\nthe way books, newspapers, and academic materials were produced across different regions.',\n",
       " 'Today, digital platforms continue this\\ntradition, offering new tools that transform how information is shared, accessed, and preserved for future generations.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2da7e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8de303",
   "metadata": {},
   "source": [
    "NLP_Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c84af3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\python versions\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python versions\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python versions\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python versions\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python versions\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python versions\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\python versions\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python versions\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python versions\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python versions\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\python versions\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\python versions\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python versions\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python versions\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python versions\\lib\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in c:\\python versions\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\python versions\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\khanh\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python versions\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python versions\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\python versions\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python versions\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\python versions\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\khanh\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\python versions\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python versions\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python versions\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python versions\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python versions\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python versions\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python versions\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\khanh\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\python versions\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\python versions\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\python versions\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\python versions\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\python versions\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\python versions\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\python versions\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\python versions\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python versions\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\khanh\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python versions\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python versions\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b974412",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d73a0238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ebb5f",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "359ef1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9385c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7a95294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73ceb6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"walk walks walking walked\"\n",
    "word_stemmer(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08283734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy\n",
      "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
      " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like\n",
      " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the\n",
      " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy\n",
    "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
    " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like\n",
    " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the\n",
    " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6617c698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0962f03",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4af80365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "730b3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00fa44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "punctuations=\"?:!.,;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa55cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa764843",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ade6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9dde468",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy\n",
    "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
    " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like\n",
    " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the\n",
    " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5771369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0640e302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['probably', 'my', 'alltime', 'favorite', 'movie', 'a', 'story', 'of', 'selflessness', 'sacrifice', 'and', 'dedication', 'to', 'a', 'noble', 'cause', 'but', 'it', 'not', 'preachy', 'or', 'boring', 'it', 'just', 'never', 'get', 'old', 'despite', 'my', 'having', 'seen', 'it', 'some', '15', 'or', 'more', 'time', 'in', 'the', 'last', '25', 'year', 'paul', 'lukas', 'performance', 'brings', 'tear', 'to', 'my', 'eye', 'and', 'bette', 'davis', 'in', 'one', 'of', 'her', 'very', 'few', 'truly', 'sympathetic', 'role', 'is', 'a', 'delight', 'the', 'kid', 'are', 'a', 'grandma', 'say', 'more', 'like', 'dressedup', 'midget', 'than', 'child', 'but', 'that', 'only', 'make', 'them', 'more', 'fun', 'to', 'watch', 'and', 'the', 'mother', 'slow', 'awakening', 'to', 'whats', 'happening', 'in', 'the', 'world', 'and', 'under', 'her', 'own', 'roof', 'is', 'believable', 'and', 'startling', 'if', 'i', 'had', 'a', 'dozen', 'thumb', 'theyd', 'all', 'be', 'up', 'for', 'this', 'movie']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0bb5b",
   "metadata": {},
   "source": [
    "Applying to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6f631d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2) Remove HTML / tags\n",
    "    text = tags_remover(text)\n",
    "    \n",
    "    # 3) Remove URLs\n",
    "    text = url_remover(text)\n",
    "    \n",
    "    # 4) Convert emojis to words\n",
    "    text = emoji_converted(text)\n",
    "    \n",
    "    # 5) Remove Punctuation\n",
    "    text = punc_remover(text)\n",
    "    \n",
    "    # 6) tokenize\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 7) remove stopwords\n",
    "    words = [word for word in words if word not in stopword]\n",
    "    \n",
    "    # 8) stemming\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # 9) lemmatization\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in stems]\n",
    "    \n",
    "    # 10) back to string\n",
    "    text = \" \".join(lemmas)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9e146d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_review'] = data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf3778",
   "metadata": {},
   "source": [
    "Bow & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "787106a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(docs):\n",
    "    vocab={}\n",
    "    i =0\n",
    "    for review in docs:\n",
    "        for word in review:\n",
    "            if word not in vocab:\n",
    "                vocab[word]=i\n",
    "                i+=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b51be",
   "metadata": {},
   "source": [
    "Now, we have vreated the bag of words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "baf0d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bow(vect, vocab):\n",
    "    inverse_vocab = {index: word for word, index in vocab.items()}\n",
    "    words_in_order = [inverse_vocab[i] for i in range(len(vocab))]\n",
    "    word_count_pairs = list(zip(words_in_order, vect))\n",
    "    sorted_word_count = sorted(word_count_pairs, key=lambda item: item[1], reverse=True)\n",
    "    print(\"Word Frequencies (Sorted):\")\n",
    "    for word, count in sorted_word_count:\n",
    "        print(f\" '{word}': {int(count)} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc04c7",
   "metadata": {},
   "source": [
    "Now, we compute TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4566b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_compute (text):\n",
    "    tf = {}\n",
    "    for word in text:\n",
    "        tf[word] = tf.get(word, 0)+1\n",
    "    total = len(text)\n",
    "    for word in tf:\n",
    "        tf[word] = tf[word]/total\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1652a1",
   "metadata": {},
   "source": [
    "Then we compute IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97946fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_idf(data_frame):\n",
    "    N = len(data_frame)\n",
    "    idf = {}\n",
    "    for document in data_frame:\n",
    "        unique_words = set(document)\n",
    "        for word in unique_words:\n",
    "            idf[word] = idf.get(word, 0) + 1\n",
    "    for word,df in idf.items():\n",
    "        idf[word] = math.log(N/df)\n",
    "        \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dba39",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "728e25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(text, idf):\n",
    "    tf = tf_compute(text)\n",
    "    tfidf = {}\n",
    "    for word in tf:\n",
    "        tfidf[word] = tf[word]*idf.get(word, 0)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cdb55e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['cleaned_review'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ffaee",
   "metadata": {},
   "source": [
    "Pre_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6704cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "    data['cleaned_review'],\n",
    "    data['sentiment'],\n",
    "    test_size = 0.2, \n",
    "    random_state= 42,\n",
    "    stratify = data['sentiment']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8cac2",
   "metadata": {},
   "source": [
    "1 which is using the BOW & TF-IDF we wrote from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "edcf7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90992176",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f85ff1",
   "metadata": {},
   "source": [
    "2 Using scikit-learn‚Äôs TfidfVectorizer instead of manual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0bdbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdb6d1",
   "metadata": {},
   "source": [
    "Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1508de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ae8eb",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e4d382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LogisticRegression(max_iter=2000))])\n",
    "lr.fit(x_train, y_train)\n",
    "lr_pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de024ae9",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cde52128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\n",
    "svm.fit(x_train, y_train)\n",
    "svm_pred = svm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1158a3",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a96536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "nb.fit(x_train, y_train)\n",
    "nb_pred = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67faa1a5",
   "metadata": {},
   "source": [
    "Testing & Determining Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6af134ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR AccuracyL 0.8916\n",
      "SVM Accuracy: 0.8943\n",
      "NB Accuracy: 0.8612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"LR AccuracyL\", accuracy_score(y_test, lr_pred))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"NB Accuracy:\", accuracy_score(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "34353cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Accuracy ===\n",
      "\n",
      "89.42999999999999\n",
      "\n",
      "=== Classification Report ===\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.89      0.89      5000\n",
      "    positive       0.89      0.90      0.90      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "\\=== Confusion Matrix ===\n",
      "\n",
      "[[4433  567]\n",
      " [ 490 4510]]\n"
     ]
    }
   ],
   "source": [
    "# SVM Best Model\n",
    "\n",
    "print(\"\\n=== Accuracy ===\\n\")\n",
    "print(accuracy_score(y_test, svm_pred)*100) ,'%'\n",
    "\n",
    "print(\"\\n=== Classification Report ===\\n\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "print(\"\\=== Confusion Matrix ===\\n\")\n",
    "print(confusion_matrix(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bdc2cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to data_tokens.npz\n",
      "x_train shape: (40000,)\n",
      "y_train shape: (40000,)\n"
     ]
    }
   ],
   "source": [
    "x_train_array = x_train.values\n",
    "x_test_array = x_test.values\n",
    "\n",
    "np.savez(\n",
    "    'data_tokens.npz',\n",
    "    x_train = x_train_array,\n",
    "    y_train = y_train,\n",
    "    x_test = x_test_array,\n",
    "    y_test = y_test\n",
    ")\n",
    "\n",
    "print(\"Data exported to data_tokens.npz\")\n",
    "print(f\"x_train shape: {x_train_array.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-kernel",
   "language": "python",
   "name": "anaconda-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
